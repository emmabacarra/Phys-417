{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ebaca\\\\Desktop\\\\Phys 417\\\\Final Project - HEP Tagging'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing libraries & making torch.device object for GPU\n",
    "\n",
    "# # neural network packages\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import Transformer\n",
    "# from torch import Tensor\n",
    "# from torch.utils.data import DataLoader\n",
    "# sys.path.append('..\\\\..\\\\PHYS417_Project')\n",
    "# from nnrunner import NetRunner\n",
    "\n",
    "# # data packages\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import pandas as pd\n",
    "# import sklearn.preprocessing as prep\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils import shuffle\n",
    "# import fndict as fd\n",
    "\n",
    "# # visual packages\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# import tqdm\n",
    "# import warnings\n",
    "\n",
    "# # Create a torch.device object to tell pytorch where to store your tensors: cpu or gpu\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Tag</u>: the type of the original particles involved in the collision\n",
    "\n",
    "<u>Jet</u>: collection of particles that hadronized (decayed) together into a stable particle\n",
    "\n",
    "Per jet, variable number of constituents (rows) with 5 features (columns):\n",
    "1. $p_T$: transverse momentum as a fraction of the jet total\n",
    "2. $\\eta$: angular coordinate relative to jet center\n",
    "3. $\\phi$: angular coordinate relative to jet center\n",
    "4. $E$: energy from constituent\n",
    "5. $\\Delta R = \\sqrt{\\eta^2 + \\phi^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization: Numerical data should be on a similar scale, typically between 0 and 1. This will help the model learn more efficiently. Use scaling such as Min-Max Scaling or Z-score normalization techniques\n",
    "\n",
    "Imbalance: If some target classes have significantly more instances than others, the model may become biased towards the majority class. Try over-sampling the minority class or under-sampling the majority class to address this.\n",
    "\n",
    "Sequence Length Uniformity: Since the sequences have varying lengths, padding or truncation will help to standardize them. This will ensure that the transformer model can handle the input effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- STEP 1: establishing training features (x) and training targets (y) data -----------------------\n",
    "# print(\"--Establishing Training Features/Targets--\")\n",
    "\n",
    "# # while training, model enters in the training features (x) and evaluates against the training targets (y)\n",
    "# trfeat = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['x']\n",
    "# # the testing data is used to evaluate the model's performance after training for predictions on unseen data\n",
    "# trtarget = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['y']\n",
    "\n",
    "# print(\"trfeat:\", trfeat.shape, \"\\ntrtarget:\", trtarget.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # ---- STEP 2: normalizing/shaping data  --------------------------------------------------------------\n",
    "# print(\"\\n --Normalizing/Shaping Data--\")\n",
    "\n",
    "# # Reducing to 2D for scaling, then reshaping back to 3D afterwards\n",
    "# scaler = prep.StandardScaler()\n",
    "\n",
    "# # traing features\n",
    "# trfeat_2d = trfeat.reshape((trfeat.shape[0], -1))\n",
    "# trfeat_2d = scaler.fit_transform(trfeat_2d)\n",
    "# trfeat = trfeat_2d.reshape(trfeat.shape)\n",
    "# print(\"trfeat reduced, reshaped:\", trfeat_2d.shape, trfeat.shape)\n",
    "\n",
    "# # training targets\n",
    "# trtarget_2d = trtarget.reshape((trtarget.shape[0], -1))\n",
    "# trtarget_2d = scaler.fit_transform(trtarget_2d)\n",
    "# trtarget = trtarget_2d.reshape(trtarget.shape)\n",
    "# print(\"trtarget reduced, reshaped:\", trtarget_2d.shape, trtarget.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # ---- STEP 3: splitting data into training, validation, and testing sets -----------------------------\n",
    "# print(\"\\n --Splitting Data and Sending to GPU--\")\n",
    "# # the validation data is used to visualize/evaluate the model's performance throughout training to help with tuning hyperparameters\n",
    "\n",
    "# # shuffling for random selection\n",
    "# trfeat, trtarget = shuffle(trfeat, trtarget, random_state=0) \n",
    "\n",
    "\n",
    "# # splitting data into training, testing, and validation sets\n",
    "# trfeat = trfeat[1000:] \n",
    "# print(\"trfeat:\", trfeat.shape)\n",
    "\n",
    "# trtarget = trtarget[1000:]\n",
    "# print(\"trtarget:\", trtarget.shape)\n",
    "\n",
    "# vafeat = trfeat[:1000] \n",
    "# print(\"vafeat:\", vafeat.shape)\n",
    "\n",
    "# vatarget = trtarget[:1000] \n",
    "# print(\"vatarget:\", vatarget.shape)\n",
    "\n",
    "\n",
    "# # sending data to GPU\n",
    "# with warnings.catch_warnings(): # booo warnings\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#     trfeat = torch.tensor(torch.from_numpy(trfeat), dtype=torch.float32).to(DEVICE)\n",
    "#     trtarget = torch.tensor(torch.from_numpy(trtarget), dtype=torch.float32).to(DEVICE)\n",
    "#     vafeat = torch.tensor(torch.from_numpy(vafeat), dtype=torch.float32).to(DEVICE)\n",
    "#     vatarget = torch.tensor(torch.from_numpy(vatarget), dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- STEP 1: establishing training features (x) and training targets (y) data -----------------------\n",
    "# print(\"--Establishing Training Features/Targets--\")\n",
    "# # while training, model enters in the training features (x) and evaluates against the training targets (y)\n",
    "\n",
    "# # event data/properties represented by a N x 5 matrix (N = variable number of constituents)\n",
    "# x1 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['x']\n",
    "# x2 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_2_tiled.npz')['x']\n",
    "# x = np.concatenate((x1, x2), axis=0)\n",
    "\n",
    "# # jet type identifier of a single 5 x 1 binary matrix (0 = no, 1 = yes)\n",
    "# y1 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['y']\n",
    "# y2 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_2_tiled.npz')['y']\n",
    "# y = np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "# print(\"features (x):\", x.shape, \"\\ntargets (y):\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # ---- STEP 2: splitting data into training, validation, and testing sets -----------------------------\n",
    "# print(\"\\n --Splitting Data--\")\n",
    "\n",
    "# # shuffling for random selection, setting seed to 0 for reproduceability below\n",
    "# x, y = shuffle(x, y, random_state=0) \n",
    "\n",
    "# # splitting data into training and testing sets\n",
    "# # the testing data is used to evaluate the model's performance after training for predictions on unseen data\n",
    "# # test_size = 0.2 means 20% of the data is for testing and 80% for training\n",
    "# trfeat, tefeat, trtarget, tetarget = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# # splitting data again to get validation set\n",
    "# # the validation data is used to visualize/evaluate performance during training to help with tuning \n",
    "# # 0.25 x 0.8 = 0.2\n",
    "# trfeat, vafeat, trtarget, vatarget = train_test_split(trfeat, trtarget, test_size=0.25, random_state=0) \n",
    "\n",
    "# print(\"Training set:\", trfeat.shape, trtarget.shape)\n",
    "# print(\"Validation set:\", vafeat.shape, vatarget.shape)\n",
    "# print(\"Testing set:\", tefeat.shape, tetarget.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # ---- STEP 3: normalizing/shaping data  --------------------------------------------------------------\n",
    "# print(\"\\n --Normalizing/Shaping Data and Sending to GPU--\")\n",
    "\n",
    "# # Reducing to 2D for scaling, then reshaping back to 3D afterwards\n",
    "# scaler = prep.StandardScaler()\n",
    "\n",
    "# # training features\n",
    "# trfeat_2d = trfeat.reshape((trfeat.shape[0], -1))\n",
    "# scaler.fit(trfeat_2d)\n",
    "# trfeat_2d = scaler.transform(trfeat_2d)\n",
    "# trfeat = trfeat_2d.reshape(trfeat.shape)\n",
    "# print(\"trfeat reduced, reshaped:\", trfeat_2d.shape, trfeat.shape)\n",
    "\n",
    "# # validation features\n",
    "# vafeat_2d = vafeat.reshape((vafeat.shape[0], -1))\n",
    "# vafeat_2d = scaler.transform(vafeat_2d)\n",
    "# vafeat = vafeat_2d.reshape(vafeat.shape)\n",
    "# print(\"vafeat reduced, reshaped:\", vafeat_2d.shape, vafeat.shape)\n",
    "\n",
    "# # testing features\n",
    "# tefeat_2d = tefeat.reshape((tefeat.shape[0], -1))\n",
    "# tefeat_2d = scaler.transform(tefeat_2d)\n",
    "# tefeat = tefeat_2d.reshape(tefeat.shape)\n",
    "# print(\"tefeat reduced, reshaped:\", tefeat_2d.shape, tefeat.shape)\n",
    "\n",
    "\n",
    "# # sending data to GPU\n",
    "# with warnings.catch_warnings(): # booo warnings\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#     trfeat = torch.tensor(torch.from_numpy(trfeat), dtype=torch.float32).to(DEVICE)\n",
    "#     trtarget = torch.tensor(torch.from_numpy(trtarget), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "#     vafeat = torch.tensor(torch.from_numpy(vafeat), dtype=torch.float32).to(DEVICE)\n",
    "#     vatarget = torch.tensor(torch.from_numpy(vatarget), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "#     tefeat = torch.tensor(torch.from_numpy(tefeat), dtype=torch.float32).to(DEVICE)\n",
    "#     tetarget = torch.tensor(torch.from_numpy(tetarget), dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # object with data for later\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, features, targets):\n",
    "#         self.features = features\n",
    "#         self.targets = targets\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.features[idx], self.targets[idx]\n",
    "\n",
    "# # Create the dataset\n",
    "# trainers = MyDataset(trfeat, trtarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- BUILDING THE MODEL -----------------------------\n",
    "# class ParticleClassifier(nn.Module):\n",
    "#     def __init__(self, num_layers, dim_model, num_heads, dim_feedforward, dropout=0.1):\n",
    "#         super(ParticleClassifier, self).__init__()\n",
    "\n",
    "#         self.transformer = nn.Transformer(d_model = dim_model, \n",
    "#                                           nhead = num_heads, \n",
    "#                                           num_encoder_layers = num_layers, \n",
    "#                                           num_decoder_layers = num_layers, \n",
    "#                                           dim_feedforward = dim_feedforward, \n",
    "#                                           dropout = dropout)\n",
    "#         self.linear = nn.Linear(dim_model, 5)  # 5 for the number of final state particles\n",
    "\n",
    "#     def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "#         src = src.permute(1, 0, 2)  # Transformer expects src to be of shape (sequence length, batch size, features)\n",
    "#         out = self.transformer(src=src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "#         out = out.permute(1, 0, 2)  # Convert back to (batch size, sequence length, features)\n",
    "#         out = self.linear(out[:, -1])  # Use the last output only\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # ---- INITIALIZING MODEL -----------------------------\n",
    "# classifier = ParticleClassifier(\n",
    "#     num_layers = 2, \n",
    "#     dim_model = trfeat.shape[1], # embedded dimension must be divisible by num_heads\n",
    "#     num_heads = fd.highest_divisor(trfeat.shape[1]), \n",
    "#     dim_feedforward = 512 \n",
    "#     ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparameters</b>\n",
    "\n",
    "Fair warning: you might get an \"out of memory\" error when training. If that happens, try reducing the batch size\n",
    "\n",
    "betas are hyperparameters that control the exponential moving averages\n",
    "\n",
    "eps is a small constant added to improve numerical stability by preventing division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 70\n",
    "# batch_size = 15\n",
    "# learning_rate = 0.002\n",
    "# betas = (0.9, 0.98)\n",
    "# eps = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Begin Training with Training Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trlosses = []\n",
    "# valosses = []\n",
    "\n",
    "# trDataLoader = DataLoader(trainers, batch_size=batch_size)\n",
    "\n",
    "# from timeit import default_timer as timer\n",
    "# train = fd.trainer\n",
    "\n",
    "# for epoch in range(1, epochs+1):\n",
    "    \n",
    "#     classifier.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     start_time = timer()\n",
    "#     for batch in trDataLoader:\n",
    "        \n",
    "#         inputs, targets = batch\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward pass\n",
    "#         output = classifier(inputs)\n",
    "        \n",
    "#         loss = loss_fn(output, targets.long())\n",
    "\n",
    "#         # backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#     #---------------------------#\n",
    "#     end_time = timer()\n",
    "    \n",
    "#     avg_loss = total_loss / len(trDataLoader)\n",
    "#     trlosses.append(avg_loss)\n",
    "\n",
    "\n",
    "#     val_loss = train.evaluate(classifier, batch_size, vafeat, loss_fn)\n",
    "#     valosses.extend(val_loss)\n",
    "\n",
    "#     print(f\"Epoch: {epoch}, Loss: {avg_loss}, Time: {(end_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.simpleload(np.array(trfeat.cpu()), np.array(trtarget.cpu()))\n",
    "# runner = NetRunner(withCuda=torch.cuda.is_available())\n",
    "# runner.fit(classifier, lr=0.0001, epochs=epochs, optimizer='adam', lossFunc='cross_entropy')\n",
    "# runner.train(batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Visualizing the Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
