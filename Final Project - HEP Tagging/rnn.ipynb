{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries & making torch.device object for GPU\n",
    "\n",
    "# neural network packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import fndict as fd\n",
    "\n",
    "# visual packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import warnings\n",
    "\n",
    "# Create a torch.device object to tell pytorch where to store your tensors: cpu or gpu\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- STEP 1: establishing training features (x) and training targets (y) data -----------------------\n",
    "print(\"--Establishing Training Features/Targets--\")\n",
    "# while training, model enters in the training features (x) and evaluates against the training targets (y)\n",
    "\n",
    "''' michael's data extraction: 10,000 final state particles per file and scaled with standard scaler '''\n",
    "\n",
    "# event data/properties represented by a N x 5 matrix (N = variable number of constituents)\n",
    "x1 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1.npz')['x'] \n",
    "x2 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_2.npz')['x']\n",
    "x = np.concatenate((x1, x2), axis=0)\n",
    "\n",
    "# jet type identifier of a single 5 x 1 binary matrix (0 = no, 1 = yes)\n",
    "y1 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1.npz')['y']\n",
    "y2 = np.load('..\\\\..\\\\PHYS417_Project\\\\data_2.npz')['y']\n",
    "y = np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "print(\"features (x):\", x.shape, \"\\ntargets (y):\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ---- STEP 2: splitting data into training, validation, and testing sets -----------------------------\n",
    "print(\"\\n --Splitting Data--\")\n",
    "\n",
    "# shuffling for random selection, setting seed to 0 for reproduceability below\n",
    "x, y = shuffle(x, y, random_state=0) \n",
    "\n",
    "# splitting data into training and testing sets\n",
    "# the testing data is used to evaluate the model's performance after training for predictions on unseen data\n",
    "# test_size = 0.2 means 20% of the data is for testing and 80% for training\n",
    "trfeat, tefeat, trtarget, tetarget = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# splitting data again to get validation set\n",
    "# the validation data is used to visualize/evaluate performance during training to help with tuning \n",
    "# 0.25 x 0.8 = 0.2\n",
    "trfeat, vafeat, trtarget, vatarget = train_test_split(trfeat, trtarget, test_size=0.25, random_state=0) \n",
    "\n",
    "print(\"Training set:\",   trfeat.shape, trtarget.shape)\n",
    "print(\"Validation set:\", vafeat.shape, vatarget.shape)\n",
    "print(\"Testing set:\",    tefeat.shape, tetarget.shape)\n",
    "\n",
    "\n",
    "''' DATA PRE-SCALED, NO NEED TO NORMALIZE '''\n",
    "\n",
    "\n",
    "\n",
    "# ---- STEP 3: sending to GPU  --------------------------------------------------------------\n",
    "print(\"\\n --Sending to GPU--\")\n",
    "\n",
    "with warnings.catch_warnings(): # booo warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    trfeat = torch.tensor(torch.from_numpy(trfeat), dtype=torch.float32).to(DEVICE)\n",
    "    trtarget = torch.tensor(torch.from_numpy(trtarget), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    vafeat = torch.tensor(torch.from_numpy(vafeat), dtype=torch.float32).to(DEVICE)\n",
    "    vatarget = torch.tensor(torch.from_numpy(vatarget), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    tefeat = torch.tensor(torch.from_numpy(tefeat), dtype=torch.float32).to(DEVICE)\n",
    "    tetarget = torch.tensor(torch.from_numpy(tetarget), dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader collate function for variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [seq.shape[0] for seq in sequences]\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_sequences, labels, lengths\n",
    "\n",
    "train_data = list(zip(trfeat, trtarget))\n",
    "val_data = list(zip(vafeat, vatarget))\n",
    "test_data = list(zip(tefeat, tetarget))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# Define the model\n",
    "class ParticleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ParticleClassifier, self).__init__()\n",
    "        self.cnn = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = x.permute(0, 2, 1)  # Convert to (batch_size, input_dim, sequence_length) for CNN\n",
    "        x = torch.relu(self.cnn(x))\n",
    "        x = x.permute(0, 2, 1)  # Convert back to (batch_size, sequence_length, hidden_dim) for LSTM\n",
    "        \n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.tanh(self.attention(output))\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.sum(attn_weights * output, dim=1)\n",
    "        \n",
    "        out = self.fc(attn_output)\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ParticleClassifier(input_dim=5, hidden_dim=64, output_dim=5).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for sequences, labels, lengths in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences.float(), lengths)\n",
    "            labels = labels.squeeze().float()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * sequences.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels, lengths in val_loader:\n",
    "                outputs = model(sequences.float(), lengths)\n",
    "                labels = labels.squeeze().float()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * sequences.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels, lengths in test_loader:\n",
    "            outputs = model(sequences.float(), lengths)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1).squeeze()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Train and evaluate\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "batch_size = 15\n",
    "learning_rate = 0.01 - 9e-6\n",
    "# learning_rate = 0.002\n",
    "betas = (0.9, 0.98)\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "# trfeat = trfeat.float()\n",
    "trtarget = trtarget.long() # convert to 64-bit integer for CrossEntropyLoss\n",
    "\n",
    "# vafeat = vafeat.float()\n",
    "vatarget = vatarget.long()\n",
    "\n",
    "# tefeat = tefeat.float()\n",
    "# tetarget = tetarget.long()\n",
    "\n",
    "# trmask = trmask[:batch_size].transpose(0, 1)\n",
    "# vamask = vamask[:batch_size].transpose(0, 1)\n",
    "\n",
    "print(trmask.shape, vamask.shape)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate, betas=betas, eps=eps)\n",
    "\n",
    "# Training Loop ---------------------------------------------------------------------------------------\n",
    "\n",
    "trlosses = []\n",
    "valosses = []\n",
    "\n",
    "print(f'Learning Rate: {learning_rate} \\nBatch Size: {batch_size} \\nEpochs: {epochs} \\n')\n",
    "with warnings.catch_warnings(): # booo warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    start_time = timer()\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_outputs = classifier(trfeat)\n",
    "        \n",
    "        loss = loss_fn(train_outputs, trtarget)\n",
    "        \n",
    "        trlosses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute Validation Accuracy ----------------------------------------------------------------------\n",
    "        \n",
    "        with torch.no_grad(): # Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "            \n",
    "            validation_outputs = classifier(vafeat)\n",
    "            \n",
    "            correct = (torch.argmax(validation_outputs, dim=1) == \n",
    "                    vatarget).type(torch.FloatTensor)\n",
    "            \n",
    "            valosses.append(correct.mean())\n",
    "    end_time = timer()\n",
    "    time = end_time - start_time\n",
    "    print(f\"Total Train Time: {'{:.0f}m {:.1f}s'.format(*divmod(time, 60))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = classifier(tefeat)\n",
    "_ignore_, predicted = torch.max(test_outputs, 1)\n",
    "correct = (predicted == tetarget).float()\n",
    "accuracy = correct.mean().item()\n",
    "# print(f'Test Accuracy: {accuracy*100:.3f}%')\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(trlosses, linewidth = 3)\n",
    "# plt.plot(runner.losses, linewidth = 3)\n",
    "plt.ylabel(\"Losses in Training\")\n",
    "plt.annotate(f'Learning Rate: {learning_rate} \\nBatch Size: {batch_size} \\nLowest Loss: {min(trlosses):.3f}', \n",
    "             xy=(0.95, 0.85), xycoords='axes fraction', va='top', ha='right')\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(valosses, linewidth = 3, color = 'gold')\n",
    "# plt.plot(runner.accuracies, linewidth = 3, color = 'gold')\n",
    "plt.ylabel(\"Training Accuracy (Validation)\")\n",
    "plt.annotate(f'Accuracy: {accuracy*100:.3f}%', xy=(0.95, 0.20), xycoords='axes fraction', va='top', ha='right')\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
