{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ebaca\\\\Desktop\\\\Phys 417\\\\Final Project - HEP Tagging'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries & sending tensors to GPU\n",
    "\n",
    "# neural network packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append('..\\\\..\\\\PHYS417_Project')\n",
    "from nnrunner import NetRunner\n",
    "\n",
    "# data packages\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import fndict as fd\n",
    "\n",
    "# visual packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Create a torch.device object to tell pytorch where to store your tensors: cpu or gpu\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Tag</u>: the type of the original particles involved in the collision\n",
    "\n",
    "<u>Jet</u>: collection of particles that hadronized (decayed) together into a stable particle\n",
    "\n",
    "Per jet, variable number of constituents (rows) with 5 features (columns):\n",
    "1. $p_T$: transverse momentum as a fraction of the jet total\n",
    "2. $\\eta$: angular coordinate relative to jet center\n",
    "3. $\\phi$: angular coordinate relative to jet center\n",
    "4. $E$: energy from constituent\n",
    "5. $\\Delta R = \\sqrt{\\eta^2 + \\phi^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Establishing Training Features/Targets--\n",
      "trfeat: (10000, 188, 5) \n",
      "trtarget: (10000, 5)\n",
      "\n",
      " --Normalizing/Shaping Data--\n",
      "trfeat reduced, reshaped: (10000, 940) (10000, 188, 5)\n",
      "trtarget reduced, reshaped: (10000, 5) (10000, 5)\n",
      "\n",
      " --Splitting Data and Sending to GPU--\n",
      "trfeat: (9000, 188, 5)\n",
      "trtarget: (9000, 5)\n",
      "vafeat: (1000, 188, 5)\n",
      "vatarget: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# ---- STEP 1: establishing training features (x) and training targets (y) data -----------------------\n",
    "print(\"--Establishing Training Features/Targets--\")\n",
    "\n",
    "# while training, model enters in the training features (x) and evaluates against the training targets (y)\n",
    "trfeat = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['x']\n",
    "# the testing data is used to evaluate the model's performance after training for predictions on unseen data\n",
    "trtarget = np.load('..\\\\..\\\\PHYS417_Project\\\\data_1_tiled.npz')['y']\n",
    "\n",
    "print(\"trfeat:\", trfeat.shape, \"\\ntrtarget:\", trtarget.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ---- STEP 2: normalizing/shaping data  --------------------------------------------------------------\n",
    "print(\"\\n --Normalizing/Shaping Data--\")\n",
    "\n",
    "# Reducing to 2D for scaling, then reshaping back to 3D afterwards\n",
    "scaler = prep.StandardScaler()\n",
    "\n",
    "# traing features\n",
    "trfeat_2d = trfeat.reshape((trfeat.shape[0], -1))\n",
    "trfeat_2d = scaler.fit_transform(trfeat_2d)\n",
    "trfeat = trfeat_2d.reshape(trfeat.shape)\n",
    "print(\"trfeat reduced, reshaped:\", trfeat_2d.shape, trfeat.shape)\n",
    "\n",
    "# training targets\n",
    "trtarget_2d = trtarget.reshape((trtarget.shape[0], -1))\n",
    "trtarget_2d = scaler.fit_transform(trtarget_2d)\n",
    "trtarget = trtarget_2d.reshape(trtarget.shape)\n",
    "print(\"trtarget reduced, reshaped:\", trtarget_2d.shape, trtarget.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ---- STEP 3: splitting data into training, validation, and testing sets -----------------------------\n",
    "print(\"\\n --Splitting Data and Sending to GPU--\")\n",
    "# the validation data is used to visualize/evaluate the model's performance throughout training to help with tuning hyperparameters\n",
    "\n",
    "# shuffling for random selection\n",
    "trfeat, trtarget = shuffle(trfeat, trtarget, random_state=0) \n",
    "\n",
    "\n",
    "# splitting data into training, testing, and validation sets\n",
    "trfeat = trfeat[1000:] \n",
    "print(\"trfeat:\", trfeat.shape)\n",
    "\n",
    "trtarget = trtarget[1000:]\n",
    "print(\"trtarget:\", trtarget.shape)\n",
    "\n",
    "vafeat = trfeat[:1000] \n",
    "print(\"vafeat:\", vafeat.shape)\n",
    "\n",
    "vatarget = trtarget[:1000] \n",
    "print(\"vatarget:\", vatarget.shape)\n",
    "\n",
    "\n",
    "# sending data to GPU\n",
    "with warnings.catch_warnings(): # booo warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    trfeat = torch.tensor(torch.from_numpy(trfeat), dtype=torch.float32).to(DEVICE)\n",
    "    trtarget = torch.tensor(torch.from_numpy(trtarget), dtype=torch.float32).to(DEVICE)\n",
    "    vafeat = torch.tensor(torch.from_numpy(vafeat), dtype=torch.float32).to(DEVICE)\n",
    "    vatarget = torch.tensor(torch.from_numpy(vatarget), dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object with data for later\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Create the dataset\n",
    "trainers = MyDataset(trfeat, trtarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ebaca\\anaconda3\\envs\\Phys417\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# ---- BUILDING THE MODEL -----------------------------\n",
    "class ParticleClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, dim_model, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super(ParticleClassifier, self).__init__()\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model = dim_model, \n",
    "                                          nhead = num_heads, \n",
    "                                          num_encoder_layers = num_layers, \n",
    "                                          num_decoder_layers = num_layers, \n",
    "                                          dim_feedforward = dim_feedforward, \n",
    "                                          dropout = dropout)\n",
    "        self.linear = nn.Linear(dim_model, 5)  # 5 for the number of final state particles\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src = src.permute(1, 0, 2)  # Transformer expects src to be of shape (sequence length, batch size, features)\n",
    "        out = self.transformer(src=src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        out = out.permute(1, 0, 2)  # Convert back to (batch size, sequence length, features)\n",
    "        out = self.linear(out[:, -1])  # Use the last output only\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---- INITIALIZING MODEL -----------------------------\n",
    "classifier = ParticleClassifier(\n",
    "    num_layers = 2, \n",
    "    dim_model = trfeat.shape[1], # embedded dimension must be divisible by num_heads\n",
    "    num_heads = fd.highest_divisor(trfeat.shape[1]), \n",
    "    dim_feedforward = 512 \n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "# betas are hyperparameters that control the exponential moving averages\n",
    "# eps is a small constant added to improve numerical stability by preventing division by zero\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Begin Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trlosses = []\n",
    "valosses = []\n",
    "\n",
    "trDataLoader = DataLoader(trainers, batch_size=batch_size)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "train = fd.trainer\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    start_time = timer()\n",
    "    for batch in trDataLoader:\n",
    "        \n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = classifier(inputs)\n",
    "\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    #---------------------------#\n",
    "    end_time = timer()\n",
    "    \n",
    "    avg_loss = total_loss / len(trDataLoader)\n",
    "    trlosses.extend(avg_loss)\n",
    "\n",
    "\n",
    "    val_loss = train.evaluate(classifier)\n",
    "    valosses.extend(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {avg_loss}, Time: {(end_time - start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.simpleload(np.array(trfeat.cpu()), np.array(trtarget.cpu()))\n",
    "# runner = NetRunner(withCuda=torch.cuda.is_available())\n",
    "# runner.fit(classifier, lr=0.0001, epochs=epochs, optimizer='adam', lossFunc='cross_entropy')\n",
    "# runner.train(batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Visualizing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(runner.losses, linewidth = 3)\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(runner.accuracies, linewidth = 3, color = 'gold')\n",
    "plt.ylabel(\"validation accuracy\")\n",
    "sns.despine()\n",
    "\n",
    "#evaluate model performance against loaded test data\n",
    "tspred, testacc, idxs = runner.eval()\n",
    "print(testacc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
